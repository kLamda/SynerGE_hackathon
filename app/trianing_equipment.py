# -*- coding: utf-8 -*-
"""trianing_equipment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DHqWJrMsFHnKEK7crZlQV8cuG5wG6Ugq
"""

import torch
import torch.nn as nn
import numpy as np
import functools
from torch.nn import init
import numpy as np
import torch
import torch.nn as nn
from torch.optim import lr_scheduler

import matplotlib.pyplot as plt
import matplotlib.animation as animation
from IPython.display import HTML
import torchvision.utils as vutils
import matplotlib.pyplot as plt

import torch
import numpy as np
import torch.nn as nn
from PIL import Image
import torchvision.transforms as T
import torchvision
from sklearn.preprocessing import StandardScaler,MinMaxScaler
import pandas as pd
import torch.nn.functional as F 
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import torch
import torch.nn as nn
from torch.autograd import Variable
from sklearn.preprocessing import MinMaxScaler
from django.contrib.staticfiles.storage import staticfiles_storage




seq_length=7

class LSTM(nn.Module):

    def __init__(self, num_classes, input_size, hidden_size, num_layers,device,cnn=False,bi=False):
        super(LSTM, self).__init__()
        
        self.num_classes = num_classes
        self.num_layers = num_layers
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.seq_length = seq_length
        self.chck=False
        self.bi=bi
        if cnn== True:
          self.chck=True
          self.seq_length=self.seq_length-1
        self.cnn=nn.Conv1d(input_size,input_size,kernel_size=2)
        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,
                            num_layers=num_layers, batch_first=True,bidirectional=self.bi)
        self.fc1=nn.Linear(hidden_size,64)
        self.fc = nn.Linear(64, num_classes)
        self.device=device

    def forward(self, x):
        if self.chck== True:
          x=x.permute(0, 2, 1) 
          x=self.cnn(x)
          x= x.permute(0, 2, 1) 
        h_0 = Variable(torch.zeros(
            self.num_layers, x.size(0), self.hidden_size))
        c_0 = Variable(torch.zeros(
            self.num_layers, x.size(0), self.hidden_size))
        if self.bi==True:
          h_0 = Variable(torch.zeros(
            2*self.num_layers, x.size(0), self.hidden_size))
          c_0 = Variable(torch.zeros(
            2*self.num_layers, x.size(0), self.hidden_size))
        
        h_0=h_0.to(self.device)
        
        c_0=c_0.to(self.device)
        # Propagate input through LSTM
        ula, (h_out, _) = self.lstm(x, (h_0, c_0))
        
        h_out = h_out.view(-1, self.hidden_size)
        
        out = self.fc(self.fc1(h_out))
        #out = self.fc1(h_out)
        
        return out



def save_checkpoint(model, opt, path ):

    
    
    save_path = path

    

    print("SAVE MODEL to", save_path)


    param_dict = {
        
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': opt.state_dict()}
        

    torch.save(param_dict, save_path)




def load_model(load_path,model,opt):

    checkpoint = torch.load(load_path,map_location=torch.device('cpu'))

    
   

    model =model # code for model init
    model.load_state_dict(checkpoint['model_state_dict'])
    

    opt =opt #code for opt init
    opt.load_state_dict(checkpoint['optimizer_state_dict'])
    #opt.param_groups[0]['lr'] = checkpoint['optimizer_params']['lr']
    


    return model, opt



def train_equi(path):
  seq_length=7
  input_size = 1
  hidden_size = 10
  num_layers = 2
  device='cpu'
  num_classes = 1
  criterion = torch.nn.MSELoss()
  model=LSTM(num_classes, input_size, hidden_size, num_layers,device,cnn=True,bi=True)
  optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)

  '''PUT PATHS OF WEIGHT FILES IN ORDER BELOW'''
  lst=[staticfiles_storage.path('equi_weights/surgery_trend.th'),staticfiles_storage.path('equi_weights/scan_trend.th'),
       staticfiles_storage.path('equi_weights/ventilator_trend.th'),staticfiles_storage.path('equi_weights/microscope_trend.th')]
  scl=[5,4,4,4]
  arr=[]
  for i,a in enumerate(lst):
    '''PUT PATH OF test_equi.csv BELOW'''
    dat=pd.read_csv(path)
    sh=np.array(dat).shape[0]
    y=dat.iloc[sh-1,i+1:i+2]  
    y=torch.Tensor(np.array(y[0])).unsqueeze(0)
    dat=dat.iloc[sh-9:sh-2,i+1:i+2]
    dat=torch.Tensor(np.array(dat)).unsqueeze(0)
    model1,optimizer1=load_model(a,model,optimizer)
    model1.train()
    for q in range(5):

      out=model1(dat)
      optimizer1.zero_grad()
      loss = criterion(out.sum(axis=0).unsqueeze(0), y)
      loss.backward()
      optimizer.step()
      print("Training : Epoch = {} loss = {}".format(q, loss))
    save_checkpoint(model1,optimizer1,a)
  return

